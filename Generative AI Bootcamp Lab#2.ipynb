{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4ba72d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb)\n",
    "\n",
    "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n",
    "\n",
    "# Getting Started with Chains\n",
    "\n",
    "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
    "\n",
    "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain ‚Äî a `PromptTemplate` ‚Äî to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain ‚Äî a LLM.\n",
    "\n",
    "We'll start by importing all the libraries that we'll be using in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb9c2a",
   "metadata": {
    "id": "66fb9c2a"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wPdWz1IdxyBR",
   "metadata": {
    "id": "wPdWz1IdxyBR"
   },
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v86cmyppxdfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v86cmyppxdfc",
    "outputId": "a897108c-be81-49b8-8802-d71741243e43"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=OPENAI_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309g_2pqxzzB",
   "metadata": {
    "id": "309g_2pqxzzB"
   },
   "source": [
    "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DsC3szr6yP3L",
   "metadata": {
    "id": "DsC3szr6yP3L"
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f31b4",
   "metadata": {
    "id": "6e1f31b4"
   },
   "source": [
    "## What are chains anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b919c3a",
   "metadata": {
    "id": "5b919c3a"
   },
   "source": [
    "**Definition**: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n",
    "\n",
    "The official definition of chains is the following:\n",
    "\n",
    "\n",
    "> A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains.\n",
    "\n",
    "\n",
    "So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4644b2f",
   "metadata": {
    "id": "c4644b2f"
   },
   "source": [
    "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
    "\n",
    "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
    "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d283b6",
   "metadata": {
    "id": "e4d283b6"
   },
   "source": [
    "Let's take a peek into what these chains have to offer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831827b7",
   "metadata": {
    "id": "831827b7"
   },
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66e4b4",
   "metadata": {
    "id": "6c66e4b4"
   },
   "source": [
    "Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HF3XCWD2sVi0",
   "metadata": {
    "id": "HF3XCWD2sVi0"
   },
   "source": [
    "#### Pro-tip: use `verbose=True` to see what the different steps in the chain are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4161561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4161561",
    "outputId": "16486831-80a5-41df-906e-376575b7f514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
      "```python\n",
      "import math\n",
      "print(math.pow(13, .3432))\n",
      "```\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 272 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198eebb2",
   "metadata": {
    "id": "198eebb2"
   },
   "source": [
    "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0821a",
   "metadata": {
    "id": "a7a0821a"
   },
   "source": [
    "**Enter prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c5798",
   "metadata": {
    "id": "c86c5798"
   },
   "source": [
    "The question we send as input to the chain is not the only input that the llm recieves üòâ. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62778ef4",
   "metadata": {
    "id": "62778ef4",
    "outputId": "211670a8-db56-4f68-d873-97d279870890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are GPT-3, and you can't do math.\n",
      "\n",
      "You can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\n",
      "\n",
      "So we hooked you up to a Python 3 kernel, and now you can execute code. If anyone gives you a hard math problem, just use this format and we‚Äôll take care of the rest:\n",
      "\n",
      "Question: ${{Question with hard calculation.}}\n",
      "```python\n",
      "${{Code that prints what you need to know}}\n",
      "```\n",
      "```output\n",
      "${{Output of your code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Otherwise, use this simpler format:\n",
      "\n",
      "Question: ${{Question without hard calculation}}\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "\n",
      "```python\n",
      "print(37593 * 67)\n",
      "```\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708031d8",
   "metadata": {
    "id": "708031d8"
   },
   "source": [
    "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! üßê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66b92768",
   "metadata": {
    "id": "66b92768",
    "outputId": "6c9b7f59-529d-409e-8562-5a622f326473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 17 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n2.907'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147e7bf",
   "metadata": {
    "id": "d147e7bf"
   },
   "source": [
    "Wrong answer! Herein lies the power of prompting and one of our most important insights so far: \n",
    "\n",
    "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2a31f",
   "metadata": {
    "id": "1cd2a31f"
   },
   "source": [
    "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3488c5b6",
   "metadata": {
    "id": "3488c5b6",
    "outputId": "8b32a998-8e11-48bf-f21c-f5d086186508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
      "        llm_executor = LLMChain(prompt=self.prompt, llm=self.llm)\n",
      "        python_executor = PythonREPL()\n",
      "        self.callback_manager.on_text(inputs[self.input_key], verbose=self.verbose)\n",
      "        t = llm_executor.predict(question=inputs[self.input_key], stop=[\"```output\"])\n",
      "        self.callback_manager.on_text(t, color=\"green\", verbose=self.verbose)\n",
      "        t = t.strip()\n",
      "        if t.startswith(\"```python\"):\n",
      "            code = t[9:-4]\n",
      "            output = python_executor.run(code)\n",
      "            self.callback_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n",
      "            self.callback_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n",
      "            answer = \"Answer: \" + output\n",
      "        elif t.startswith(\"Answer:\"):\n",
      "            answer = t\n",
      "        else:\n",
      "            raise ValueError(f\"unknown format from LLM: {t}\")\n",
      "        return {self.output_key: answer}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b6c2e",
   "metadata": {
    "id": "fa6b6c2e"
   },
   "source": [
    "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96bd3",
   "metadata": {
    "id": "67f96bd3"
   },
   "source": [
    "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109619a",
   "metadata": {
    "id": "b109619a"
   },
   "source": [
    "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly üî•\n",
    "\n",
    "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html) and tinker with the example notebooks that you might find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e329c",
   "metadata": {
    "id": "381e329c"
   },
   "source": [
    "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a25a2",
   "metadata": {
    "id": "f66a25a2"
   },
   "source": [
    "### Generic chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b32a84",
   "metadata": {
    "id": "70b32a84"
   },
   "source": [
    "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e2048",
   "metadata": {
    "id": "4b8e2048"
   },
   "source": [
    "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e778d2",
   "metadata": {
    "id": "a6e778d2"
   },
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c794e00a",
   "metadata": {
    "id": "c794e00a"
   },
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc1ac6",
   "metadata": {
    "id": "42dc1ac6"
   },
   "source": [
    "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "286f7295",
   "metadata": {
    "id": "286f7295"
   },
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "977bf11a",
   "metadata": {
    "id": "977bf11a",
    "outputId": "8d6eaa5e-b417-4c17-a345-7b7e32071430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84cd0",
   "metadata": {
    "id": "b3f84cd0"
   },
   "source": [
    "Great! Now things will get interesting.\n",
    "\n",
    "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77042a",
   "metadata": {
    "id": "5b77042a"
   },
   "source": [
    "First we will build the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73719a5d",
   "metadata": {
    "id": "73719a5d"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2ec83",
   "metadata": {
    "id": "83b2ec83"
   },
   "source": [
    "And next, initialize our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48a067ab",
   "metadata": {
    "id": "48a067ab"
   },
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324005d",
   "metadata": {
    "id": "2324005d"
   },
   "source": [
    "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
    "\n",
    "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da4925",
   "metadata": {
    "id": "c5da4925"
   },
   "source": [
    "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06f51f17",
   "metadata": {
    "id": "06f51f17"
   },
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f51d8",
   "metadata": {
    "id": "7f0f51d8"
   },
   "source": [
    "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8032489",
   "metadata": {
    "id": "a8032489"
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f55d21",
   "metadata": {
    "id": "b2f55d21"
   },
   "source": [
    "We are all set. Time to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d507aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 163 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nChains let us link up multiple pieces to make one dope app. Like, we can take user input, style it up with a PromptTemplate, then pass it to an LLM. We can get even more creative by combining multiple chains or mixin' chains with other components.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b52e19",
   "metadata": {
    "id": "60b52e19"
   },
   "source": [
    "## A note on langchain-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f649da",
   "metadata": {
    "id": "02f649da"
   },
   "source": [
    "`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "411500c2",
   "metadata": {
    "id": "411500c2"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375e5b7",
   "metadata": {
    "id": "b375e5b7"
   },
   "source": [
    "Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbe8748d",
   "metadata": {
    "id": "fbe8748d"
   },
   "outputs": [],
   "source": [
    "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfe67c",
   "metadata": {
    "id": "ebcfe67c"
   },
   "source": [
    "What if we want to change some of the configuration parameters? We can simply override it after loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0d54233",
   "metadata": {
    "id": "d0d54233",
    "outputId": "92eba1cf-e47b-4df1-cc51-caee5a6a720b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "074f8806",
   "metadata": {
    "id": "074f8806"
   },
   "outputs": [],
   "source": [
    "llm_math_chain.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "465a6cbf",
   "metadata": {
    "id": "465a6cbf",
    "outputId": "0207bf08-0db0-4d85-e3b9-ac7922f344c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc688ca",
   "metadata": {},
   "source": [
    "That's it for this example on chains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541eb2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b61a3-92eb-4891-90ee-1d10607b05ad",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336d784-65c2-4a11-8489-b445b1fad177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a09c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943237a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdb42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abc20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44d1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febee243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d5b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1eb2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78458efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c129ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016187ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0730e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accf92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a46121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89603117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b04f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade83f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f590e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b06fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f50bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefec24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98018a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2e2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387109d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7d560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b2131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b717379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5be01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f0121-cf7b-464d-bb3d-6357719188ed",
   "metadata": {},
   "source": [
    "Reminder: Download your notebook to you local computer to save your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcqKO0aI6_PI",
   "metadata": {
    "id": "hcqKO0aI6_PI"
   },
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# Conversational Memory\n",
    "\n",
    "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
    "\n",
    "The memory allows a _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* ‚Äî meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
    "\n",
    "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
    "\n",
    "In this notebook we'll explore this form of memory in the context of the LangChain library.\n",
    "\n",
    "We'll start by importing all of the libraries that we'll be using in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uZR3iGJJtdDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZR3iGJJtdDE",
    "outputId": "98873b1a-5688-4f64-c400-e17be707c56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m344.0/344.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m785.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a90e762",
   "metadata": {
    "id": "66fb9c2a"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "                                                  ConversationSummaryMemory, \n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43637a2d",
   "metadata": {
    "id": "wPdWz1IdxyBR"
   },
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02c4fa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c02c4fa2",
    "outputId": "ed941db8-a50d-4e7d-d302-7b6b8c371c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44baa9ae",
   "metadata": {
    "id": "baaa74b8"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='text-davinci-003'  # can be used with llms like 'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f96120",
   "metadata": {
    "id": "309g_2pqxzzB"
   },
   "source": [
    "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "094c1fe5",
   "metadata": {
    "id": "DsC3szr6yP3L"
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CnNF6i9r8RY_",
   "metadata": {
    "id": "CnNF6i9r8RY_"
   },
   "source": [
    "Now let's dive into **Conversational Memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2aff9",
   "metadata": {
    "id": "6e1f31b4"
   },
   "source": [
    "## What is memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9ba62",
   "metadata": {
    "id": "5b919c3a"
   },
   "source": [
    "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
    "\n",
    "The official definition of memory is the following:\n",
    "\n",
    "\n",
    "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of ‚ÄúMemory‚Äù exists to do exactly that.\n",
    "\n",
    "\n",
    "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343a0e2",
   "metadata": {
    "id": "3343a0e2"
   },
   "source": [
    "Before we delve into the different memory modules that the library offers, we will introduce the chain we will be using for these examples: the `ConversationChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c13e9",
   "metadata": {
    "id": "6c9c13e9"
   },
   "source": [
    "As always, when understanding a chain it is interesting to peek into its prompt first and then take a look at its `._call` method. As we saw in the chapter on chains, we can check out the prompt by accessing the `template` within the `prompt` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ff1ce3",
   "metadata": {
    "id": "96ff1ce3"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90ad394d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ad394d",
    "outputId": "1c641d37-b3e7-40d5-815b-936fcd2d9a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b1e0c",
   "metadata": {
    "id": "9f8b1e0c"
   },
   "source": [
    "Interesting! So this chain's prompt is telling it to chat with the user and try to give truthful answers. If we look closely, there is a new component in the prompt that we didn't see when we were tinkering with the `LLMMathChain`: _history_. This is where our memory will come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e7770",
   "metadata": {
    "id": "4a7e7770"
   },
   "source": [
    "What is this chain doing with this prompt? Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43bfd2da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43bfd2da",
    "outputId": "489437a5-0f0b-412a-f817-f0df817211c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        known_values = self.prep_inputs(inputs.copy())\n",
      "        return self.apply([known_values])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e664af",
   "metadata": {
    "id": "84e664af"
   },
   "source": [
    "Nothing really magical going on here, just a straightforward pass through an LLM. In fact, this chain inherits these methods directly from the `LLMChain` without any modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8f4aa79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8f4aa79",
    "outputId": "ca3413ec-1ceb-4160-f6e9-2031350780a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        known_values = self.prep_inputs(inputs.copy())\n",
      "        return self.apply([known_values])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa70bf",
   "metadata": {
    "id": "6aaa70bf"
   },
   "source": [
    "So basically this chain combines an input from the user with the conversation history to generate a meaningful (and hopefully truthful) response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5172f",
   "metadata": {
    "id": "19f5172f"
   },
   "source": [
    "Now that we've understood the basics of the chain we'll be using, we can get into memory. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a33f6",
   "metadata": {
    "id": "0f1a33f6"
   },
   "source": [
    "## Memory types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d732b7a",
   "metadata": {
    "id": "4d732b7a"
   },
   "source": [
    "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d70642",
   "metadata": {
    "id": "04d70642"
   },
   "source": [
    "### Memory type #1: ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3cb2b",
   "metadata": {
    "id": "53d3cb2b"
   },
   "source": [
    "The `ConversationBufferMemory` does just what its name suggests: it keeps a buffer of the previous conversation excerpts as part of the context in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a974a",
   "metadata": {
    "id": "d80a974a"
   },
   "source": [
    "**Key feature:** _the conversation buffer memory keeps the previous pieces of conversation completely unmodified, in their raw form._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2267f1f0",
   "metadata": {
    "id": "2267f1f0"
   },
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lseziAMcAyvX",
   "metadata": {
    "id": "lseziAMcAyvX"
   },
   "source": [
    "We pass a user prompt the the `ConversationBufferMemory` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "M0cwooC5A5Id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0cwooC5A5Id",
    "outputId": "8a8178eb-b9ac-45cf-baed-255b413b0630"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today, isn't it? How can I help you?\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlKINTFYA9eo",
   "metadata": {
    "id": "xlKINTFYA9eo"
   },
   "source": [
    "This one call used a total of `85` tokens, but we can't see that from the above. If we'd like to count the number of tokens being used we just pass our conversation chain object and the message we'd like to input via the `count_tokens` function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1bd5a88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "d1bd5a88",
    "outputId": "cb593afd-7efd-4c0e-cf04-82dc1a324aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 179 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "146170ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "146170ca",
    "outputId": "dbb6f78c-b169-463e-c1c8-a35151894f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 268 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Well, integrating Large Language Models with external knowledge can open up a lot of possibilities. For example, you could use them to generate more accurate and detailed summaries of text, or to answer questions about a given context more accurately. You could also use them to generate more accurate translations, or to generate more accurate predictions about future events.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e15411a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "3e15411a",
    "outputId": "f6857844-ee6f-49ef-df50-54335f248bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 360 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'  There are a variety of data sources that could be used to give context to a Large Language Model. These include structured data sources such as databases, unstructured data sources such as text documents, and even audio and video data sources. Additionally, you could use external knowledge sources such as Wikipedia or other online encyclopedias to provide additional context.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3352cc48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "3352cc48",
    "outputId": "62294954-cc7e-4ef3-e5fc-19a5c4ffc4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 388 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Your aim is to explore the potential of integrating Large Language Models with external knowledge.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b74ff",
   "metadata": {
    "id": "431b74ff"
   },
   "source": [
    "Our LLM with `ConversationBufferMemory` can clearly remember earlier interactions in the conversation. Let's take a closer look to how the LLM is saving our previous conversation. We can do this by accessing the `.buffer` attribute for the `.memory` in our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "984afd09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "984afd09",
    "outputId": "4233d17f-1001-48e5-d256-0595e00dbf40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Is there anything else I can help you with?\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI:  Well, integrating Large Language Models with external knowledge can open up a lot of possibilities. For example, you could use them to generate more accurate and detailed summaries of text, or to answer questions about a given context more accurately. You could also use them to generate more accurate translations, or to generate more accurate predictions about future events.\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "AI:   There are a variety of data sources that could be used to give context to a Large Language Model. These include structured data sources such as databases, unstructured data sources such as text documents, and even audio and video data sources. Additionally, you could use external knowledge sources such as Wikipedia or other online encyclopedias to provide additional context.\n",
      "Human: What is my aim again?\n",
      "AI:  Your aim is to explore the potential of integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570267d",
   "metadata": {
    "id": "4570267d"
   },
   "source": [
    "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1a90b",
   "metadata": {
    "id": "acf1a90b"
   },
   "source": [
    "### Memory type #2: ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f61fe9",
   "metadata": {
    "id": "01f61fe9"
   },
   "source": [
    "The problem with the `ConversationBufferMemory` is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516c7d4",
   "metadata": {
    "id": "0516c7d4"
   },
   "source": [
    "Enter `ConversationSummaryMemory`.\n",
    "\n",
    "Again, we can infer from the name what is going on.. we will keep a summary of our previous conversation snippets as our history. How will we summarize these? LLM to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a905",
   "metadata": {
    "id": "86b0a905"
   },
   "source": [
    "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized form, where the summarization is performed by an LLM._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6050c",
   "metadata": {
    "id": "0ea6050c"
   },
   "source": [
    "In this case we need to send the llm to our memory constructor to power its summarization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f33a16a7",
   "metadata": {
    "id": "f33a16a7"
   },
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c4896",
   "metadata": {
    "id": "b64c4896"
   },
   "source": [
    "When we have an llm, we always have a prompt ;) Let's see what's going on inside our conversation summary memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c476824d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c476824d",
    "outputId": "282be20e-9048-4f37-fc89-8a7eb8dfe1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90cdf3",
   "metadata": {
    "id": "df90cdf3"
   },
   "source": [
    "Cool! So each new interaction is summarized and appended to a running summary as the memory of our chain. Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34343665",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "34343665",
    "outputId": "ac04f6bc-9dcb-446c-d4b9-8fd2311d605e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 290 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b757bba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "b757bba3",
    "outputId": "9de1823a-0dfe-45ff-fadc-26eff6fdce99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 440 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" That sounds like an interesting project! I'm familiar with Large Language Models, but I'm not sure how they could be integrated with external knowledge. Could you tell me more about what you have in mind?\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0a373e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "d0a373e2",
    "outputId": "d4f561d7-d1c7-45e5-99ba-266130ee67ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 664 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' I can think of a few possibilities. One option is to use a large language model to generate a set of candidate answers to a given query, and then use external knowledge to filter out the most relevant answers. Another option is to use the large language model to generate a set of candidate answers, and then use external knowledge to score and rank the answers. Finally, you could use the large language model to generate a set of candidate answers, and then use external knowledge to refine the answers.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e286f0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "2e286f0d",
    "outputId": "9558ef92-5f9c-4818-be8b-1e7e6ec19864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 799 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' There are many different types of data sources that could be used to give context to the model. These could include structured data sources such as databases, unstructured data sources such as text documents, or even external APIs that provide access to external knowledge. Additionally, the model could be trained on a combination of these data sources to provide a more comprehensive understanding of the context.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "891180f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "891180f2",
    "outputId": "8035333e-d7c0-4a46-d8b8-acb3501d27e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 853 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Your aim is to explore the potential of integrating Large Language Models with external knowledge.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d768e44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d768e44",
    "outputId": "3bd42ac9-d56b-45f4-99ac-45cd5a656b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI with a good morning, to which the AI responded with a good morning and asked how it could help. The human expressed interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responded positively and asked for more information. The human asked the AI to think of different possibilities, and the AI suggested three options: using the large language model to generate a set of candidate answers and then using external knowledge to filter out the most relevant answers, score and rank the answers, or refine the answers. The human then asked which data source types could be used to give context to the model, to which the AI responded that there are many different types of data sources that could be used, such as structured data sources, unstructured data sources, or external APIs. Additionally, the model could be trained on a combination of these data sources to provide a more comprehensive understanding of the context. The human then asked what their aim was again, to which the AI responded that their aim was to explore the potential of integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd35c8c",
   "metadata": {
    "id": "0dd35c8c"
   },
   "source": [
    "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
    "\n",
    "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "nzijj4RZFX3I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzijj4RZFX3I",
    "outputId": "dc272cbb-acfd-4b4a-f854-8fa63f9732d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 334\n",
      "Summary memory conversation length: 219\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('text-davinci-003')\n",
    "\n",
    "# show number of tokens for the memory used by each memory type\n",
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab0c09",
   "metadata": {
    "id": "2bab0c09"
   },
   "source": [
    "_Practical Note: the `text-davinci-003` and `gpt-3.5-turbo` models [have](https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens) a large max tokens count of 4096 tokens between prompt and answer._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494830ea",
   "metadata": {
    "id": "494830ea"
   },
   "source": [
    "### Memory type #3: ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00762844",
   "metadata": {
    "id": "00762844"
   },
   "source": [
    "Another great option for these cases is the `ConversationBufferWindowMemory` where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably. We will control this window with the `k` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a5915",
   "metadata": {
    "id": "206a5915"
   },
   "source": [
    "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45be373a",
   "metadata": {
    "id": "45be373a"
   },
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc4dd8a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "fc4dd8a0",
    "outputId": "c4ec1cc8-f218-4f7b-e27e-f5fb73e59228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 85 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9992e8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "b9992e8d",
    "outputId": "ac7ae1af-2329-4766-ac5e-8fce24a1d272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f2e98d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "3f2e98d9",
    "outputId": "dc60726a-4be2-480f-892b-443da9b2859e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 233 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2a8d062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "a2a8d062",
    "outputId": "dbb27cf0-2e87-41d0-a733-68921d250481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 245 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Data sources that could be used to give context to the model include text corpora, structured databases, and ontologies. Text corpora provide a large amount of text data that can be used to train the model and provide additional context. Structured databases provide structured data that can be used to provide additional context to the model. Ontologies provide a structured representation of knowledge that can be used to provide additional context to the model.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff199a3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ff199a3f",
    "outputId": "81573cf0-7f39-4a8c-8ccd-e79cd80f2523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 186 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Your aim is to use data sources to give context to the model.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f59f77",
   "metadata": {
    "id": "f5f59f77"
   },
   "source": [
    "As we can see, it effectively 'fogot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be `1`, we would expect it remembers only the last interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b354c8d",
   "metadata": {
    "id": "6b354c8d"
   },
   "source": [
    "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85266406",
   "metadata": {
    "id": "85266406"
   },
   "outputs": [],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5904ae2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5904ae2a",
    "outputId": "bd0aa797-7a43-4af5-a531-209aa6272dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my aim again?\n",
      "AI:  Your aim is to use data sources to give context to the model.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b937d",
   "metadata": {
    "id": "ae8b937d"
   },
   "source": [
    "Makes sense. \n",
    "\n",
    "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fbb50fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fbb50fe",
    "outputId": "c35dca36-a7c7-4d61-da19-c28173fa8319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 334\n",
      "Summary memory conversation length: 219\n",
      "Buffer window memory conversation length: 26\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
    "    f'Buffer window memory conversation length: {len(tokenizer.encode(bufw_history))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69842cc1",
   "metadata": {
    "id": "69842cc1"
   },
   "source": [
    "_Practical Note: We are using `k=2` here for illustrative purposes, in most real world applications you would need a higher value for k._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea5fc8",
   "metadata": {
    "id": "2aea5fc8"
   },
   "source": [
    "### More memory types!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb5162",
   "metadata": {
    "id": "daeb5162"
   },
   "source": [
    "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0365333",
   "metadata": {
    "id": "f0365333"
   },
   "source": [
    "#### ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f298e",
   "metadata": {
    "id": "317f298e"
   },
   "source": [
    "**Key feature:** _the conversation summary memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef5c8b",
   "metadata": {
    "id": "57ef5c8b"
   },
   "source": [
    "#### ConversationKnowledgeGraphMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40248f03",
   "metadata": {
    "id": "40248f03"
   },
   "source": [
    "This is a super cool memory type that was introduced just [recently](https://twitter.com/LangChainAI/status/1625158388824043522). It is based on the concept of a _knowledge graph_ which recognizes different entities and connects them in pairs with a predicate resulting in (subject, predicate, object) triplets. This enables us to compress a lot of information into highly significant snippets that can be fed into the model as context. If you want to understand this memory type in more depth you can check out [this](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph) blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91952cd1",
   "metadata": {
    "id": "91952cd1"
   },
   "source": [
    "**Key feature:** _the conversation knowledge graph memory keeps a knowledge graph of all the entities that have been mentioned in the interactions together with their semantic relationships._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02241bc3",
   "metadata": {
    "id": "02241bc3"
   },
   "outputs": [],
   "source": [
    "# you may need to install this library\n",
    "# !pip install -qU networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5f10a89",
   "metadata": {
    "id": "c5f10a89"
   },
   "outputs": [],
   "source": [
    "conversation_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65957fe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "65957fe2",
    "outputId": "c9561a4a-412a-4d92-865d-9e81a09bb101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1565 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hi Human! My name is AI. It's nice to meet you. I like mangoes too! Did you know that mangoes are a great source of vitamins A and C?\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_kg, \n",
    "    \"My name is human and I like mangoes!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74054534",
   "metadata": {
    "id": "74054534"
   },
   "source": [
    "The memory keeps a knowledge graph of everything it learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a8c54fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a8c54fb",
    "outputId": "adf96679-087b-4b77-c00d-9bf9e98f9278"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 'human', 'name'), ('human', 'mangoes', 'likes')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_kg.memory.kg.get_triples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1ca15",
   "metadata": {
    "id": "e1a1ca15"
   },
   "source": [
    "#### ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9aeaf",
   "metadata": {
    "id": "41e9aeaf"
   },
   "source": [
    "**Key feature:** _the conversation entity memory keeps a recollection of the main entities that have been mentioned, together with their specific attributes._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900a385",
   "metadata": {
    "id": "2900a385"
   },
   "source": [
    "The way this works is quite similar to the `ConversationKnowledgeGraphMemory`, you can refer to the [docs](https://python.langchain.com/en/latest/modules/memory/types/entity_summary_memory.html) if you want to see it in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45112bd",
   "metadata": {
    "id": "d45112bd"
   },
   "source": [
    "## What else can we do with memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78296bff",
   "metadata": {
    "id": "78296bff"
   },
   "source": [
    "There are several cool things we can do with memory in langchain. We can:\n",
    "* implement our own custom memory module\n",
    "* use multiple memory modules in the same chain\n",
    "* combine agents with memory and other tools\n",
    "\n",
    "If this piques your interest, we suggest you to go take a look at the memory [how-to](https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html) section in the docs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f518f5",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0924a2-ce2d-44ba-a806-c1195720c237",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad6fe2",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdf13d",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24677d",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ef937",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3339a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529400d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018cb0a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14219b70",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e9905",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61631b1f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdf9ec",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca79256",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a4497",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eeccc3",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6233e",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4553fb",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a788403",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087bc87",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faaa952",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20ddaa",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b2194",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d063c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9020ed",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43582ee6",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284288e1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcf8b1",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b238f",
   "metadata": {
    "height": 285,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ecabe",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728edba",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a221b1d",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb582617",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
